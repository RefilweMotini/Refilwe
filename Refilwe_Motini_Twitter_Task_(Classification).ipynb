{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Install NLTK toolkit for NLP tasks**``\n",
        "\n"
      ],
      "metadata": {
        "id": "FErAxsWMWSdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edN81OZlFc3Y",
        "outputId": "96b6ee74-d735-4baa-b3f4-bc2cf02f2985"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import NLTK** **toolkit**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R-CVQ8p8XAJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1wFrKR3FhD_",
        "outputId": "a3594001-41ec-4920-ed52-0c18cc3406d5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXEJnyEKFy9h",
        "outputId": "a35eb931-729b-4995-c2f3-92f8cc8fc137"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download wordnet**"
      ],
      "metadata": {
        "id": "Uk94QiAXZmjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!python -m nltk.downloader wordnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JycbqOMYGCWC",
        "outputId": "0e6f22c3-7f1b-437c-f9f8-d760add88e37"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import the necessary libraries**"
      ],
      "metadata": {
        "id": "Ag0voviwaP8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier  # Import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "vMiDNIcYZ_f6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mount the dataset utilizing google drive as storage**"
      ],
      "metadata": {
        "id": "6dO1DvB5adzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change the working directory to the location of your data\n",
        "%cd '/content/drive/My Drive/Dataset'\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gD6LnBgfabsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the dataset**"
      ],
      "metadata": {
        "id": "ai8jK-Peat1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "train_set_path = 'train.csv'\n",
        "test_set_path = 'test.csv'\n",
        "\n",
        "df1 = pd.read_csv(train_set_path)\n",
        "df2 = pd.read_csv(test_set_path)"
      ],
      "metadata": {
        "id": "JfiuBWS9arUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocess the dataset**"
      ],
      "metadata": {
        "id": "ID2DAd6ua1va"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess text data\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "    return preprocessed_text\n",
        "\n",
        "# Apply preprocessing to the text columns in df1 and df2\n",
        "df1['clean_text'] = df1['message'].apply(preprocess_text)\n",
        "df2['clean_text'] = df2['message'].apply(preprocess_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "neslxHGRa-Sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define features and target variable**"
      ],
      "metadata": {
        "id": "BL38m8CQbJvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define features and target variable\n",
        "X = df1['clean_text']\n",
        "y = df1['sentiment']\n"
      ],
      "metadata": {
        "id": "cKkXNTz9bQLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split data into training and validation sets**"
      ],
      "metadata": {
        "id": "fEf6gjZZbWAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "JWkMu4zDbfwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a pipeline with TF-IDF vectorizer and Random Forest classifier model**"
      ],
      "metadata": {
        "id": "zMJvW3BTcd5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a pipeline with TF-IDF vectorizer and Random Forest classifier model\n",
        "pipeline = make_pipeline(TfidfVectorizer(), RandomForestClassifier())"
      ],
      "metadata": {
        "id": "ZHDxsQ1Lcjxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the model**"
      ],
      "metadata": {
        "id": "_sK82YcHcvvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train the model on the training set\n",
        "pipeline.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "9H1rXhC1cv_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predict Sentiments**"
      ],
      "metadata": {
        "id": "5ihS9lVQc-5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Predict sentiment on the validation set\n",
        "valid_predictions = pipeline.predict(X_valid)"
      ],
      "metadata": {
        "id": "KJnTIjDDc40q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculate F1 score on the validation set**"
      ],
      "metadata": {
        "id": "5vs-Pde4dMbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate F1 score on the validation set\n",
        "f1 = f1_score(y_valid, valid_predictions, average='weighted')"
      ],
      "metadata": {
        "id": "DhSnFzIVdHBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the model on the entire training set**"
      ],
      "metadata": {
        "id": "3cI_wanNdfFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model on the entire training set\n",
        "pipeline.fit(X, y)"
      ],
      "metadata": {
        "id": "rh7_IRz0df1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predict sentiment on the test data**"
      ],
      "metadata": {
        "id": "f_mqGuGzdqiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict sentiment on the test data\n",
        "predictions = pipeline.predict(df2['clean_text'])"
      ],
      "metadata": {
        "id": "TS6aj2PldqzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Create a DataFrame with tweetid and sentiment**\n"
      ],
      "metadata": {
        "id": "ae_hmhfVd59E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a DataFrame with tweetid and sentiment\n",
        "submission_df = pd.DataFrame({'tweetid': df2['tweetid'], 'sentiment': predictions})"
      ],
      "metadata": {
        "id": "TNUOBBY8d6Lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save the DataFrame to a CSV file**"
      ],
      "metadata": {
        "id": "vNaUeL9leFi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "submission_df.to_csv('submission.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "rzHr4enq3f-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Print Completion message**"
      ],
      "metadata": {
        "id": "AFV8B_PvcM56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print message\n",
        "print(\"Submission file saved as 'submission.csv'. You can download it now.\")"
      ],
      "metadata": {
        "id": "ZhLLwRnPcIIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Print the F1 Score**"
      ],
      "metadata": {
        "id": "K3a1PL_WcAFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"F1 Score on Validation Set: {f1}\")"
      ],
      "metadata": {
        "id": "0OgxiK7Eb9_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot the F1 score**"
      ],
      "metadata": {
        "id": "67BgRBmhbozZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting F1 score\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.barplot(x=['F1 Score (Validation)'], y=[f1])\n",
        "plt.title('F1 Score on Validation Set')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.xlabel('Dataset')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3vESTbY7bmuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download the Submission file**"
      ],
      "metadata": {
        "id": "Y2-lfHOCbvvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the file\n",
        "files.download('submission.csv')\n"
      ],
      "metadata": {
        "id": "YMVFj0rZKhk5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}